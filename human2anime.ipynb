{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Face Transformation from Human to Anime\n",
    "This notebook demonstrates how to transform a human face to an anime face using StyleGAN2.\n",
    "\n",
    "Contact: alienncheng@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "First of all, we need a dataset to train the model.\n",
    "I downloaded the [danbooru2019-portrait](https://www.gwern.net/Crops#danbooru2019-portraits) dataset and manually pick 1000 images from it.\n",
    "\n",
    "Then we run the dataset tool provided by StyleGAN2 to create TensorFlow records.\n",
    "\n",
    "#### Note\n",
    "1. As [Doron Adler's work](https://twitter.com/Buntworthy/status/1297976798236598274) shows, 317 images should be enough.\n",
    "2. Beware of mode issues. The model never learn something not (or seldomly) appearing in the dataset. Due to the extreme gender ratio of the anime portraits, the model would eventually learn how to generate anime faces excluding male faces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python dataset_tool.py create_from_images datasets/custom_512 datasets_img/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a StyleGAN2 network to generate anime portrait\n",
    "We need a network to generate anime portrait.\n",
    "\n",
    "I suggest transfer learning from a well-trained network so that we can not only save the time but also somehow keep the latent space of the source network.\n",
    "\n",
    "Here I fine-tuned the model [stylegan2-ffhq-512-config-f](https://twitter.com/AydaoAI/status/1269689136019116032?s=20) with the dataset above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python run_training.py --result-dir=D://Anime/results/ --data-dir=datasets/ --dataset=custom_512 --config=config-f --total-kimg=200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Align a human face image\n",
    "An aligned face image is more understandable for model. Furthermore, we need the image cropped in a suitable manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python align_images.py images/raw images/aligned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have the essential materials prepared, a human face image, a ffhq model and a anime face model, both of which share the latent space (or similar at least).\n",
    "\n",
    "Then what we need to do next is to tranform the human face to an anime face.\n",
    "\n",
    "Here we have some choices to get our works done:\n",
    "1. Extract the latent code of the given human face image, then simply insert it to the anime model.\n",
    "2. Blend the human model and the anime model to get a mixture of them. That would be closer to the original image but the models might conflict and generate a hybrid.\n",
    "3. With both models generating paired images, learn a pix2pix model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract the latent code of the face image\n",
    "We need to attain the latent code corresponding to the given human image so that we can make use of it, and [rolux have done this](https://github.com/rolux/stylegan2encoder)!\n",
    "\n",
    "> Note that we should replace *training/dataset.py* by *training/dataset-toonify.py*, and replace *dataset_tool.py* by *dataset_tool-toonify.py* here. The **-s2.py* files are backups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python project_images.py --num-steps 500 images/aligned images/generated --network-pkl=pretrained_models/stylegan2-ffhq-512-config-f.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Anime face with the latent code\n",
    "We get the latent code of the given image now.\n",
    "\n",
    "The simplest way to generate the anime face image is by inserting the code directly into the anime model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In this way, we assume the latent space between the human face model and the anime face model was the same.\n",
    ">\n",
    "> However, in fact, we just fine-tuned it so we could not guarantee such assumption would hold.\n",
    ">\n",
    "> So the output image would be a little different from the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python generate_from_latent.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blend the models\n",
    "As [Justin Pinkney's work](https://colab.research.google.com/drive/1tputbmA9EaXs9HL9iO21g7xN7jz_Xrko?usp=sharing) shows, StyleGAN2 models can be blended easily.\n",
    "\n",
    "We can get a blended model to generate an image between a human face and an anime face.\n",
    "\n",
    "> Similarly we can transform a human face to a blended face.\n",
    ">\n",
    "> If you want to do so, you need to revise the file *generate_from_latent.py*.\n",
    ">\n",
    "> Replace *pretrained_models/ffhq-to-anime-512-config-f.pkl* by *pretrained_models/blended.pkl*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python blend_models.py --low_res_pkl=pretrained_models/stylegan2-ffhq-512-config-f.pkl --high_res_pkl=pretrained_models/ffhq-to-anime-512-config-f.pkl --resolution=32 --output_pkl=pretrained_models/blended.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train a pix2pix model\n",
    "WIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "[Analyzing and Improving the Image Quality of StyleGAN](https://arxiv.org/abs/1912.04958)\n",
    "\n",
    "[Toonify yourself by Justin Pinkney](https://www.justinpinkney.com/toonify-yourself/)\n",
    "\n",
    "[stylegan2encoder by rolux](https://github.com/rolux/stylegan2encoder)\n",
    "\n",
    "[Making Anime Faces With StyleGAN](https://www.gwern.net/Faces)\n",
    "\n",
    "[malnyun_faces by bryandlee](https://github.com/bryandlee/malnyun_faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
